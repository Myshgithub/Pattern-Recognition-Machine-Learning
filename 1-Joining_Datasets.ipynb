{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Section 1.1: A deep(er) dive into joining and cleaning datasets\n\n* Data scientists spend ~ 80% of the time and effort devoted to a project preparing data for anlaysis\n* Because most real-world data is split among multiple datasets\n * this invariably means cleaning and joining diffent datasets together\n* Thus, mastering these skills is essential for undertaking data science.\n\n* We will use __`pandas`__, the principal Python library for data handling and manipulation\n\n* In order to provide an experience more like real-world data science, we will use real data taken gathered from the [U.S. Department of Agriculture National Nutrient Database for Standard Reference](https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/nutrient-data-laboratory/docs/usda-national-nutrient-database-for-standard-reference/)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Reminders about importing, built-in Help, and documentation\n\nThe standard convention in Python-centric data science is to import pandas under the alias __`pd`__, which is what we will use here:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* __`pandas`__ is a big package and there's a lot to keep track of\n* Fortunately, Jupyter gives you the ability to quickly explore the contents of a package like pandas by using its tab-completion feature\n* If you want to see all of the functions available with pandas, type this:\n\n```ipython\nIn [2]: pd.<TAB>\n\n```\n\nWhen you do so, a drop-down menu will appear next to the `pd`.\n\n> **Exercise**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Place your cursor after the period and press <TAB>:\npd.\n\n# Now select an item using tab-completion and then add a period\n# and use tab-completion to explore *that*.\n# For example, you could try placing pressing <TAB> after:\n# pd.DataFrame.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* In addition to the tab-completion feature (above) you can access help with the __`?`__ (question mark) character\n* If you don't understand something about a function you see in this section, consulting the docs can help!\n* You'll find this documentation to be a very valuable reference source for your own data science work\n* As a reminder, use this code to display the built-in pandas documentation:\n\n```ipython\nIn [4]: pd?\n```\n\n> **Exercise**\n\n> Run this code cell and review the documentation for the pandas DataFrame object. We are going to use it quite a bit."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## A brief reminder about Jupyter notebooks\n\n* This course makes extensive use of Jupyter notebooks hosted on Microsoft Azure\n* ...making it easy to experiment with programming concepts in an interactive fashion\n* Jupyter notebooks are divided into cells\n* Each cell either contains text written in the Markdown markup language or a space in which to write and execute code\n* Let's take a brief tour around our Jupyter notebook...\n\n> **Note**: This notebook is designed to have you run cells one by one, and several code cells contain deliberate errors for demonstration purposes\n> * Therefore, if you use the **Cell** > **Run All** command, some code cells past the error won't be run. To resume running the code in each case, use **Cell** > **Run All Below** from the cell after the error."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Loading data\n\n> **Learning goal:** By the end of this subsection, you should be comfortable loading data from files into a __`DataFrame`__ and troubleshooting any difficulties that might arise.\n\n* a __`DataFrame`__ is a two-dimensional data structures\n * similar to flat-file formats such as comma-separated value (CSV) file (the most common import and export format for spreadsheets and databases)\n* __`pandas`__ provides a convenient function to load the contents of CSV files into a __`DataFrame`__ (more convenient, in fact, then the native Python [CSV library](https://docs.python.org/3.6/library/csv.html))\n* Let's get comfortable with [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) because we will be using often."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/USDA-nndb.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After talking up the convenience of pd.read_csv, it might seem strange that we immediately encounter an error. The clue as to what went wrong is in the last line of the error message:\n\n`UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 2: invalid continuation byte`\n\nThe data in the CSV file uses a character that the default Unicode Standard ([UTF-8](https://en.wikipedia.org/wiki/UTF-8)) codec reading this file doesn't understand. Remember, this is real-world data and the real world is a messy place.\n\nIt's time to use the pd.read_csv documentation to look for ideas on what to try next.\n\n> **Exercise**\n\n> Use the built-in IPython documentation to on `pd.read_csv.`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Hint: Refer to the discussion at the start of this section if\n# you forgot the syntax.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are quite a few of parameters for this function. The intuitively named `encoding` parameter accepts `str` input from the list of Python [standard encodings](https://docs.python.org/3.6/library/codecs.html#standard-encodings). We will go with `'latin_1'` here.\n\n**Note:** Although data-science practitioners do develop a familiarity with different encodings they have encountered, selecting the correct encoding can sometimes come down to trial and error, even for professionals!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/USDA-nndb.csv', encoding='latin_1')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There was no error message this time, so `'latin_1'` did the trick and we successfully read in the CSV file to the `df` `DataFrame`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** There is a saying that the difference between data science in academia and the real world is that academia likes to do complex analysis on clean datasets, whereas the real world often does simpler analysis on messier datasets. Troubleshooting difficulties — even ones encountered while merely loading your data — is a large part of successful data science."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Exploring and cleaning the data\n\n> **Learning goal:** By the end of this subsection, you should be comfortable performing simple exploration of your data and performing simple cleaning steps on it to prepare it for later analysis.\n\n* Let's now take a look at our `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* We can get some more specific information about the `DataFrame` by using its `info()` method:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* quickly inspecting the columns from __`df`__ we can see that almost all the columns have a lot of null values\n * missing values are not an issue for us right now, but they will pose a challenge in future sections (but we will deal with them in those sections)\n\n* Let's also check to see if this __`DataFrame`__ has any duplicate values in it\n * We do this by looking at the __`NDB_No`__ column, which serves as the unique identifier for all foods in the National Nutrient Database for Standard Reference"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.duplicated('NDB_No').sum() # let's be sure we understand this",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* Given the nature of the data source (a government reference database) it makes sense that there are no duplicate entries\n* For purposes of learning more about cleaning data, let's duplicate data by using the __`append()``__ method"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = df.append(df, ignore_index=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* The __`append()`__ method has basically stacked the __`DataFrame`__ by appending a copy of __`df`__ to the end of the __`DataFrame`__\n * __`ignore_index=True`__ means the index numbering for the newly doubled `DataFrame` continues seamlessly.\n\n* Let's look directly at how many times individual values in a column (such as __`NDB_No`__) are duplicated using the __`pivot_table`__ method for the __`DataFrame`__:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dups = df.pivot_table(index=['NDB_No'], aggfunc='size')\nprint(dups)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Let's try another way... value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> You can create a spreadsheet-style pivot table on this DataFrame by using more than one column. Can you figure out how to do this for the __`NDB_No`__ and __`Shrt_Desc`__ columns?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Hint: refer to the documentation using df.pivot_table? if you need help.\ndf.pivot_table(columns=('NDB_No', 'Shrt_Desc'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* Given that we duplicated the original dataset, two duplicates of everything is not unexpected\n* However, these duplicate values will pose a problem for us later in the section if not dealt with, so let's take care of them now:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = df.drop_duplicates('NDB_No', keep='last')\ndf.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The `DataFrame` is now half of its original size, which is what we would expect. However, look at this line in the `df.info()` output:\n\n`Int64Index: 8790 entries, 8790 to 17579`\n\nWhile there are only now 8790 entries per column, the indexing for the DataFrame does not run 0 through 8789, as we might have expected. We can see this more directly by looking at the `head` of the redacted `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> Is this behavior of the `drop_duplicates()` method not updating the index values of the `DataFrame` surprising or unexpected for you? Can you explain why this method behaves as it does in this case? If not, study the documentation for this method by using `df.drop_duplicates?` in the code cell below until you're satisfied with your understanding of this behavior."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# We could also use reset_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Duplicate, `Null`, and `NaN` values can all complicate (if not derail) your analysis. Learning how to identify and remove these problems is a huge part of successfully performing data science."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Splitting the `DataFrame`\n\n> **Learning goal:** By the end of this subsection, you should be comfortable selecting and dropping specific columns from a __`DataFrame`__\n\n* It might seem strange to discuss splitting a __`DataFrame`__ in a course about joining them, but we'll do so here to create the __`DataFrame`__s that we'll join later on\n\n* we take this approach for two reasons:\n\n1. Creating our own __`DataFrame`__s gives us easy control over the content of the child __`DataFrame`__s to best demonstrate aspects of joining datasets\n2. Because we have a baseline, joined __`DataFrame` (`df`)__, it's easy to see how different methods of joining the child __`DataFrame`__s produce different results.\n\n* We'll create two child `DataFrame`s from `df`: `df1` and `df2`. First, we'll create `df1` from the first 35 columns of `df`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = df.iloc[:,:35]\ndf1.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n\n> Why did we use syntax `df1 = df.iloc[:,:35]` to capture the first 35 columns of `df`? What does the first `:` (colon) in the square brackets do? Experiment with `df3 = df.iloc[:35]` in the code cell below and compare `df3.info()` with `df1.info()` to satisfy yourself as to why we need to use this syntax."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# you can also use .shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* We'll create `df2` in a similar manner to `df1`, but we need to do things a little differently here to ensure that the first columne (`NDB_No`) makes it into `df2`\n * This is going to serve as the column that's common to both child `DataFrame`s when we join them later in this section.\n\n* We also want to populate `df2` with a different number of rows than `df1`\n * Doing so will make is easier to demonstrate what goes on with some of the join techniques shown below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2 = df.iloc[:2000, [0] + [i for i in range(35, 53)]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n\n> If you're unsure about why we use `range(35,53)` in the list comprehension above, review the documentation for the `range()` function using `range?` in the code cell below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Remeber that Python uses zero-indexing",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can examine `df2` by using the `head()` and `info()` methods."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let’s take a look at `df1`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You'll notice on that both `DataFrame`s have their old indices indexes that they inherited from  `df`. We can fix that by using the `reset_index()` method, but then we run into a problem."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = df1.reset_index()\ndf1.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our old indexes are still there for `df1`, but now they're in a new column titled \"`index`.\" We need to address that by using the `drop=True` parameter for the method. (We also need to drop the `index` column we just created.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = df1.drop(['index'], axis=1)\ndf1 = df1.reset_index(drop=True)\ndf1.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's do the same thing to `df2`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2 = df2.reset_index(drop=True)\ndf2.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* For practice, let's export these `DataFrame`s to CSV files by using the `to_csv()` method. Note that unless we explicitly tell pandas not to, it will also export the index as a column in the CSV file. We will also need to be careful to explicitly encode our CSV to UTF-8."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1.to_csv('Data/NNDB1.csv', sep=',', encoding='utf-8',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Export df2 to a CSV file.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Although it's not common in the real world to split `DataFrame`s only to re-merge them later, you'll need to drop columns or create new `DataFrame`s that contain only the information you need. With truly large datasets, this is not just a convenience for you analysis, but a necessity for memory and performance!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Joining `DataFrame`s\n\n> **Learning goal:** By the end of this subsection, you should be comfortable performing left, right, inner, and outer merges on `DataFrame`s.\n\n* We'll examine the  most commonly used `DataFrame` function for joining datasets: `merge()`\n * But first, let's refresh ourselves on the shapes of our two `DataFrame`s so that the output of our joining makes more sense"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* The type of dataset join that’s most widely used by practicing data scientists is the _left join_\n * (if you already have some experience with SQL, you know what this refers to)\n * a left join is a join that takes all of the data from one `DataFrame` (think of it as the left set in a Venn diagram) and merges it with everything that it has in common with another `DataFrame` (the intersection with the right set in the same Venn diagram)\n\n* We do this using the `merge()` function\n * We also need to specify the type of join we want to perform by using the `how` parameter, as well as the index on which to join the `DataFrames` by using the `on` parameter"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "left_df = pd.merge(df1, df2, how='left', on='NDB_No')\nleft_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n\n> Is the shape of the resulting `DataFrame` what you were expecting? Why or why not?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's compare this to the original `df` `DataFrame`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n\n> The shapes are the same, but do you expect `df` and `left_df` to be identical? If so, why? If not, what differences do you expect there to be between them?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's check to see what the differences between these `DataFrame`s might be."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "left_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The indexes notwithstanding, the first five rows of both `DataFrame`s are the same. Let's check the last five rows."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "left_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are differences here in the last five rows. Notice that the right-most columns of `left_df` contain have Not a Number (`NaN`) values. This is because the left `DataFrame` was larger than the right `DataFrame`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n\n> A right join is simply the mirror image of a left join in which those entries from the left `DataFrame` that are common with the right `DataFrame` are merged with the right `DataFrame`.\n>\n>Perform a right join of `df1` and `df2` in the code cell below. But before you do that, ask yourself what shape you expect the resulting `DataFrame` to have? Do you expect it to have any `NaN` values?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Hint: the parameter for the right join is how='right'\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another intuitive and widely used type of join is the inner join. This join simply merges entries that are common to both `DataFrame`s, resulting in a `DataFrame` that has no `NaN` values."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inner_df = pd.merge(df1, df2, how='inner', on='NDB_No')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n\n> Before we examine the shape of the resulting `DataFrame`, what do you predict it will be? Why?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inner_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Did `inner_df` behave as you expected it would? Let's briefly examine it by using the `head()` and `tail()` methods."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inner_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inner_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* The resulting `DataFrame` is essentially the first 2000 rows of the original `df` `DataFrame`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n\n> An outer join is the union of two `DataFrame`s; anything that is in either `DataFrame` with be in the result\n* Perform an outer join of `df1` and `df2`\n* What shape do you expect the resulting `DataFrame` to have?\n* How does it differ from the right join of `df1` and `df2`?\n* What differences would there have to be in the shape or content of either `DataFrame` for the outer join of the two to be different from their right join?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Preparing for coming sections\n\nWe will be using the USDA NNDB dataset in Sections 1.2 and 1.3. However, particularly in Section 1.2, we want to include food group information to go with the food entries to aid with interpreting the result of our data analysis in that section. You will add food group information to this USDA dataset in preparation for these coming sections.\n\nFirst, let's reload our original NNDB dataset so that we have a clean copy."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/USDA-nndb.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's load in the columns that we want from the older NNDB dataset that includes food groups."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fg_df = pd.read_csv('Data/USDA-nndb-combined.csv', usecols=['NDB_No', 'FoodGroup'])\nfg_df = fg_df.rename(index=str, columns={' ID': 'NDB_No'})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fg_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that `fg_df` does not have the same number of rows as `df`:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fg_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> We need to combine `df` and `fg_df` using the pandas `merge()` function. As you prepare to do so, keep the following considerations front of mind:\n> 1. Which type of join should you use to capture all of the information in both datasets? (**Hint:** Look at the `head` and `tail` of the resulting `DataFrame` for clues.)\n> 2. In order to put the `FoodGroup` column immediately after the `NDB_No` column, in what order should you enter the two `DataFrame`s into the `merge()` function? (You might need to experiment a couple of times to get the desired order.)\n>\n> Perform the command to join the `df` and `fg_df` in the code cell below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "combined_df = pd.merge(df, fg_df, how='outer', on='NDB_No')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise solution**\n>\n> The correct code to use for this is `combined_df = pd.merge(fg_df, df, how='outer', on='NDB_No')`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "combined_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "combined_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now save the merged `DataFrame` using the `to_csv()` method."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "combined_df.to_csv('Data/USDA-nndb-merged.csv', \n                   sep=',', \n                   encoding='latin_1', \n                   index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Because the most interesting insights come from joining different datasets, the pandas `merge()` function is at the heart of most data science projects."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}